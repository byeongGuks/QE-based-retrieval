{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsMibmYsKVBc",
        "outputId": "9aa155bc-4fc9-4a02-9eaf-0204c7d3d29c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/pygaggle-master/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQA5pc5BKX6u",
        "outputId": "2c5b12a4-9fe9-4b10-b0a7-36f05a2ba0a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/pygaggle-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojypsbkgKbN9",
        "outputId": "0260c0dd-14f9-476d-8a38-b284b483dfb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
            "Collecting coloredlogs==14.0 (from -r requirements.txt (line 1))\n",
            "  Downloading coloredlogs-14.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.22.4)\n",
            "Collecting pydantic==1.9.1 (from -r requirements.txt (line 4))\n",
            "  Downloading pydantic-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyserini>=0.16.0 (from -r requirements.txt (line 5))\n",
            "  Downloading pyserini-0.21.0-py3-none-any.whl (154.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.10.1)\n",
            "Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (3.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.12.2)\n",
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.12.0)\n",
            "Collecting tokenizers==0.13.3 (from -r requirements.txt (line 11))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.56.0 (from -r requirements.txt (line 12))\n",
            "  Downloading tqdm-4.56.0-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.29.0 (from -r requirements.txt (line 13))\n",
            "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.95 (from -r requirements.txt (line 14))\n",
            "  Downloading sentencepiece-0.1.95.tar.gz (508 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.7/508.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence_transformers==2.2.2 (from -r requirements.txt (line 15))\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.0.1+cu118)\n",
            "Collecting faiss-cpu (from -r requirements.txt (line 17))\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=7.1 (from coloredlogs==14.0->-r requirements.txt (line 1))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.9.1->-r requirements.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (0.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->-r requirements.txt (line 13)) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.29.0->-r requirements.txt (line 13))\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->-r requirements.txt (line 13)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->-r requirements.txt (line 13)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0->-r requirements.txt (line 13)) (2.27.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2->-r requirements.txt (line 15)) (0.15.2+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2->-r requirements.txt (line 15)) (3.8.1)\n",
            "Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from pyserini>=0.16.0->-r requirements.txt (line 5)) (0.29.34)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyserini>=0.16.0->-r requirements.txt (line 5)) (1.5.3)\n",
            "Collecting pyjnius>=1.4.0 (from pyserini>=0.16.0->-r requirements.txt (line 5))\n",
            "  Downloading pyjnius-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nmslib>=2.1.1 (from pyserini>=0.16.0->-r requirements.txt (line 5))\n",
            "  Downloading nmslib-2.1.1.tar.gz (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime>=1.8.1 (from pyserini>=0.16.0->-r requirements.txt (line 5))\n",
            "  Downloading onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from pyserini>=0.16.0->-r requirements.txt (line 5)) (3.3.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->-r requirements.txt (line 8)) (3.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.12.2->-r requirements.txt (line 9)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.12.2->-r requirements.txt (line 9)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.12.2->-r requirements.txt (line 9)) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.12.2->-r requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.12.2->-r requirements.txt (line 9)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.12.2->-r requirements.txt (line 9)) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.12.2->-r requirements.txt (line 9)) (0.40.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 16)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 16)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 16)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->-r requirements.txt (line 16)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->-r requirements.txt (line 16)) (16.0.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.12.2->-r requirements.txt (line 9)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.12.2->-r requirements.txt (line 9)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.12.2->-r requirements.txt (line 9)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.12.2->-r requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0->-r requirements.txt (line 13)) (2023.4.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.1.0)\n",
            "Collecting pybind11<2.6.2 (from nmslib>=2.1.1->pyserini>=0.16.0->-r requirements.txt (line 5))\n",
            "  Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=2.1.1->pyserini>=0.16.0->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini>=0.16.0->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini>=0.16.0->-r requirements.txt (line 5)) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.0->-r requirements.txt (line 13)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.0->-r requirements.txt (line 13)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.0->-r requirements.txt (line 13)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.0->-r requirements.txt (line 13)) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.1->-r requirements.txt (line 8)) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.1->-r requirements.txt (line 8)) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.2.1->-r requirements.txt (line 8)) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.12.2->-r requirements.txt (line 9)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->-r requirements.txt (line 16)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers==2.2.2->-r requirements.txt (line 15)) (8.4.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.12.2->-r requirements.txt (line 9)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.12.2->-r requirements.txt (line 9)) (3.2.2)\n",
            "Building wheels for collected packages: sentencepiece, sentence_transformers, nmslib\n",
            "  Building wheel for sentencepiece (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentencepiece: filename=sentencepiece-0.1.95-cp310-cp310-linux_x86_64.whl size=1546219 sha256=03d7fb9931914f231db8de33cd288fb92c0580ff842a77f8222432dcb5eb2c76\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/a4/01/5a500fc0c5a38917ef408c245eb40b7ac96f4a30fc6a346a4c\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=0d596a0589558a872b69cf55163002db4429ba59fb0f96fb14d33f1460d3859a\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for nmslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nmslib: filename=nmslib-2.1.1-cp310-cp310-linux_x86_64.whl size=13572552 sha256=690a0d6e79a3cf5817d8ebfc7432a4f59e39893aa11f6903395a6a59f59967b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/1a/5d/4cc754a5b1a88405cad184b76f823897a63a8d19afcd4b9314\n",
            "Successfully built sentencepiece sentence_transformers nmslib\n",
            "Installing collected packages: tokenizers, sentencepiece, pyjnius, faiss-cpu, tqdm, pydantic, pybind11, humanfriendly, nmslib, huggingface-hub, coloredlogs, transformers, onnxruntime, pyserini, sentence_transformers\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.7\n",
            "    Uninstalling pydantic-1.10.7:\n",
            "      Successfully uninstalled pydantic-1.10.7\n",
            "Successfully installed coloredlogs-14.0 faiss-cpu-1.7.4 huggingface-hub-0.14.1 humanfriendly-10.0 nmslib-2.1.1 onnxruntime-1.15.0 pybind11-2.6.1 pydantic-1.9.1 pyjnius-1.5.0 pyserini-0.21.0 sentence_transformers-2.2.2 sentencepiece-0.1.95 tokenizers-0.13.3 tqdm-4.56.0 transformers-4.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89k58KtpRpsA",
        "outputId": "b5c6ba42-c4be-4bd7-f491-96c1f5fd187e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.22.4)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te3cr26qKgGF",
        "outputId": "a8cb97cc-cb77-4bae-bfaf-d4dd37d6478f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_accuracy(df_json, k=100):\n",
        "  true_cnt = 0\n",
        "  true_k_cnt = 0\n",
        "  for i in range(len(df_json)): # iterate per row\n",
        "    flag=0  \n",
        "    for j in range(len(df_json[i]['ctxs'])): # iterate per ctxs\n",
        "      if df_json[i]['ctxs'][j]['has_answer']:\n",
        "        flag=1\n",
        "        true_cnt += 1\n",
        "        break\n",
        "        \n",
        "    if flag==1:\n",
        "      for m in range(k):\n",
        "        if df_json[i]['ctxs'][m]['has_answer']:\n",
        "          true_k_cnt += 1\n",
        "          break\n",
        "  return true_k_cnt / true_cnt\n",
        "\n",
        "def calculate_precision(df_json, k=100):\n",
        "  precision = 0\n",
        "  for i in range(len(df_json)): # iterate per row\n",
        "    true_cnt = 0\n",
        "    pos_lst = [1 for x in df_json[i]['ctxs'] if x['has_answer']]\n",
        "    for j in range(k): # iterate per ctxs\n",
        "      if df_json[i]['ctxs'][j]['has_answer']:\n",
        "        true_cnt += 1\n",
        "    if len(pos_lst):\n",
        "      curr_precision = true_cnt / len(pos_lst)\n",
        "    else:\n",
        "      curr_precision = 0\n",
        "    precision += curr_precision\n",
        "  return precision / len(df_json)\n",
        "\n",
        "def calculate_mrr(df_json, k=100):\n",
        "  mrr = 0\n",
        "  for i in range(len(df_json)): # iterate per row\n",
        "    for j in range(k): # iterate per ctxs\n",
        "      if df_json[i]['ctxs'][j]['has_answer']:\n",
        "        mrr += 1 / (j + 1) # (j+1)-th rank\n",
        "        break\n",
        "  return mrr / len(df_json)\n",
        "\n",
        "def calculate_map(df_json, k=100):\n",
        "  map = 0\n",
        "  for i in range(len(df_json)): # iterate per row\n",
        "    true_cnt = 0\n",
        "    curr_precision = 0\n",
        "    for j in range(k): # iterate per ctxs\n",
        "      if df_json[i]['ctxs'][j]['has_answer']:\n",
        "        true_cnt += 1\n",
        "        curr_precision += true_cnt / (j + 1)\n",
        "    if true_cnt:\n",
        "      map += curr_precision / true_cnt\n",
        "  return map / len(df_json)\n",
        "\n",
        "def calculate_ndcg(df_json, k=100):\n",
        "  ndcg = 0\n",
        "  for i in range(len(df_json)): # iterate per row\n",
        "    dcg = 0\n",
        "    true_cnt = 0\n",
        "    ideal_dcg = 1\n",
        "    for j in range(k): # iterate per ctxs\n",
        "      if df_json[i]['ctxs'][j]['has_answer']:\n",
        "        dcg += 1 / np.log2(j + 2)\n",
        "        ideal_dcg += 1 / np.log2(true_cnt + 2)\n",
        "        true_cnt += 1\n",
        "    if ideal_dcg or dcg:\n",
        "      ndcg += dcg / ideal_dcg\n",
        "  return ndcg / len(df_json)\n"
      ],
      "metadata": {
        "id": "00hbLGMvKnAd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "def tokenizer(sent):\n",
        "  return sent.split(\" \")\n",
        "\n",
        "def reading_json_file(path):\n",
        "    queryy=[]\n",
        "    answerr=[]\n",
        "    \n",
        "    with open(path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "        print(len(data))\n",
        "        corpus = [[['a' for _ in range(4)] for _ in range(100)] for _ in range(len(data))]\n",
        "        bm25_corpus = [['a' for _ in range(100)] for _ in range(len(data))] \n",
        "        for i in range(len(data)):\n",
        "            #queryy.append(data[i][\"question\"])\n",
        "            #queryy.append(data[i][\"expanded_query\"])\n",
        "            #queryy.append(data[i][\"expanded_query_2\"])\n",
        "            #queryy.append(data[i][\"expanded_query_gpt\"])\n",
        "            queryy.append(data[i][\"expanded_query_3\"])\n",
        "            #queryy.append(data[i][\"expanded_query_4\"])\n",
        "            #answerr.append(data[i][\"answers\"]) #json 파일 answerr \n",
        "            for j in range(100):\n",
        "                corpus[i][j][0]=data[i][\"ctxs\"][j][\"id\"]\n",
        "                corpus[i][j][1]=data[i][\"ctxs\"][j][\"title\"]\n",
        "                corpus[i][j][2]=data[i][\"ctxs\"][j][\"text\"]\n",
        "                corpus[i][j][3]=data[i][\"ctxs\"][j][\"has_answer\"]\n",
        "                bm25_corpus[i][j]=corpus[i][j][2]\n",
        "    # tokenized_corpus = [tokenizer(doc) for doc in bm25_corpus]\n",
        "    # bm25 = BM25Okapi(tokenized_corpus)\n",
        "    \n",
        "    # for i in range(len(queryy)): \n",
        "    #   tokenized_query = tokenizer(queryy[i])\n",
        "    #   doc_scores = bm25.get_scores(tokenized_query)\n",
        "    #   for j in range(100): \n",
        "    #     bm25_scores.append(doc_scores[j])\n",
        "\n",
        "    # for i in range(len(corpus)):\n",
        "    #   corpus[i] = sorted(corpus[i], key=lambda x: x[3], reverse=True)\n",
        "      \n",
        "    # #query당 100개의 text의 bm25 score를 계산 -> 이를 바탕으로 sorting -> somewhat reranking \n",
        "    return queryy, answerr, corpus, data, bm25_corpus"
      ],
      "metadata": {
        "id": "ztEcGdLyKnaO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path='/content/drive/MyDrive/Colab Notebooks/NQ_test_benchmark_withgpt_deduplicated_final.json'\n",
        "queryy, answerr, corpus, data, bm25_corpus = reading_json_file(file_path)\n",
        "print(queryy[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJR92a2-KrVd",
        "outputId": "7920e88b-d5ea-4f00-d104-7a7c2086a447"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "272\n",
            "The southwest monsoon blow over Nigeria during summer\n",
            "the south west wind blows across nigeria between\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/pygaggle-master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RaTezmFekPH",
        "outputId": "89861d99-e174-466c-dceb-dd1eccb8c1ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/pygaggle-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as py \n",
        "import json \n",
        "\n",
        "def tokenizer(sent):\n",
        "    return sent.split(\" \") \n",
        "    #sent.split(\"\\n\"): 문장 전체를 받아줌 \n",
        "    #sent.split(\"\"): 단어 별로 받음 (문장 -> 단어로 쪼개짐)\n",
        "  \n",
        "bm25_scores = [] \n",
        "result = [] \n",
        "\n",
        "for q in range(len(queryy)): \n",
        "  #print(len(bm25_corpus[q]))\n",
        "  tokenized_corpus = [tokenizer(doc) for doc in bm25_corpus[q]]\n",
        "  #print(tokenized_corpus)\n",
        "  bm25 = BM25Okapi(tokenized_corpus)\n",
        "    \n",
        "  query = queryy[q]\n",
        "  #answer = answerr[q]\n",
        "  # print(answer)\n",
        "  tokenized_query = query.split(\" \")\n",
        "  # print(\"######################################\")\n",
        "  # print(tokenized_query)\n",
        "    # print(\"######################################\")\n",
        "  doc_scores = bm25.get_scores(tokenized_query)\n",
        "  \n",
        "  s = doc_scores.argsort() ##오름차순으로 정렬된 index \n",
        "  sorted_scores = doc_scores[s][::-1] #내림차순 \n",
        "  #print(sorted_scores)\n",
        "\n",
        "  # print(sorted_scores)\n",
        "  \n",
        "  sorted_id = [corpus[q][i][0] for i in s[::-1]]\n",
        "  sorted_title = [corpus[q][i][1] for i in s[::-1]]\n",
        "  sorted_text = [corpus[q][i][2] for i in s[::-1]]\n",
        "  sorted_has_answer = [corpus[q][i][3] for i in s[::-1]]  # query에 해당하는 문서들만 추출하여 정렬\n",
        "\n",
        "  ##\n",
        "  \n",
        "  # for j in range(100):\n",
        "  #   corpus[q][j][3]= sorted_has_answer\n",
        "  #   print(corpus[q][j][3])\n",
        "  # print(\"------\")\n",
        "  # corpus[q] = sorted(corpus[q], key=lambda x: doc_scores[s[::-1]].tolist().index(x['text']), reverse=True)\n",
        "  # corpus[q] = sorted(corpus[q], key=lambda x: x in doc_scores[s][::-1], reverse=True)\n",
        "  print(\"ITERATION: \", q)\n",
        "  #print(sorted_has_answer)\n",
        "  # sorted_corpus = [corpus[q][i] for i in s[::-1]]\n",
        "  # print(sorted_corpus[q][0])\n",
        "  # ctxs = [{\n",
        "  #       'id': sorted_corpus[q][i][0] for i in s[::-1],\n",
        "  #       'title': sorted_corpus[q][i][1] for i in s[::-1],\n",
        "  #       'text': sorted_corpus[q][i][2] for i in s[::-1],\n",
        "  #       'bm25_score': doc_scores[s][::-1],\n",
        "  #       'has_answer': sorted_corpus[q][i][3] for i in s[::-1] }]\n",
        "\n",
        "  # result.append({\n",
        "  #       \"question\": query,\n",
        "  #       \"answers\": answer,\n",
        "  #       \"ctxs\": ctxs\n",
        "  # })\n",
        "  for k in range(100):\n",
        "    data[q][\"ctxs\"][k][\"id\"]=sorted_id[k]\n",
        "    data[q][\"ctxs\"][k][\"title\"]=sorted_title[k]\n",
        "    data[q][\"ctxs\"][k][\"text\"]=sorted_text[k]\n",
        "    data[q][\"ctxs\"][k][\"score\"]=sorted_scores[k]\n",
        "    data[q][\"ctxs\"][k][\"has_answer\"]=sorted_has_answer[k]\n",
        "# json_data = json.dumps(result)\n",
        "\n",
        "# with open('NQ_test_benchmark_final_bm25.json', 'w') as file:\n",
        "#     file.write(json_data)\n",
        "#   # 결과를 리스트에 추가\n",
        "#   for doc in sorted_corpus:\n",
        "#     print(doc)\n",
        "  # print(\"######################################\")\n",
        "  \n",
        "  \n",
        "  # data['key'] = 'new value'\n",
        "  # print(corpus[q])\n",
        "  #   print(\"\\n\")\n",
        "\n",
        "  # print(corpus[q])\n",
        "  # # bm25_scores.append(doc_scores) #append 된거 확인 \n",
        "  # # print(bm25.get_top_n(tokenized_query))\n",
        "  # # print(bm25_scores) #100개 [0]에 100개 다 들어감 \n",
        "  # # count = 0\n",
        "  # with open('NQ_test_benchmark_final_bm25.json', 'w') as file:\n",
        "  #    json.dump(data, file)\n",
        "\n",
        "\n",
        "k = 10\n",
        "\n",
        "print(f'Accuracy@{k}: {calculate_accuracy(data, k):.4f}')\n",
        "print(f'Precision@{k}: {calculate_precision(data, k):.4f}')\n",
        "print(f'MRR@{k}: {calculate_mrr(data, k):.4f}')\n",
        "print(f'MAP@{k}: {calculate_map(data, k):.4f}')\n",
        "print(f'nDCG@{k}: {calculate_ndcg(data, k):.4f}')\n",
        "\n",
        "## json 파일명 변경 및 data range 설정 필요\n",
        "with open('NQ_test_benchmark_reranked_bm25_qe3.json', 'w') as file:\n",
        "      json.dump(data, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1sZngIXKvOu",
        "outputId": "7a854dee-08d5-437b-b90b-416526b54002"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ITERATION:  0\n",
            "ITERATION:  1\n",
            "ITERATION:  2\n",
            "ITERATION:  3\n",
            "ITERATION:  4\n",
            "ITERATION:  5\n",
            "ITERATION:  6\n",
            "ITERATION:  7\n",
            "ITERATION:  8\n",
            "ITERATION:  9\n",
            "ITERATION:  10\n",
            "ITERATION:  11\n",
            "ITERATION:  12\n",
            "ITERATION:  13\n",
            "ITERATION:  14\n",
            "ITERATION:  15\n",
            "ITERATION:  16\n",
            "ITERATION:  17\n",
            "ITERATION:  18\n",
            "ITERATION:  19\n",
            "ITERATION:  20\n",
            "ITERATION:  21\n",
            "ITERATION:  22\n",
            "ITERATION:  23\n",
            "ITERATION:  24\n",
            "ITERATION:  25\n",
            "ITERATION:  26\n",
            "ITERATION:  27\n",
            "ITERATION:  28\n",
            "ITERATION:  29\n",
            "ITERATION:  30\n",
            "ITERATION:  31\n",
            "ITERATION:  32\n",
            "ITERATION:  33\n",
            "ITERATION:  34\n",
            "ITERATION:  35\n",
            "ITERATION:  36\n",
            "ITERATION:  37\n",
            "ITERATION:  38\n",
            "ITERATION:  39\n",
            "ITERATION:  40\n",
            "ITERATION:  41\n",
            "ITERATION:  42\n",
            "ITERATION:  43\n",
            "ITERATION:  44\n",
            "ITERATION:  45\n",
            "ITERATION:  46\n",
            "ITERATION:  47\n",
            "ITERATION:  48\n",
            "ITERATION:  49\n",
            "ITERATION:  50\n",
            "ITERATION:  51\n",
            "ITERATION:  52\n",
            "ITERATION:  53\n",
            "ITERATION:  54\n",
            "ITERATION:  55\n",
            "ITERATION:  56\n",
            "ITERATION:  57\n",
            "ITERATION:  58\n",
            "ITERATION:  59\n",
            "ITERATION:  60\n",
            "ITERATION:  61\n",
            "ITERATION:  62\n",
            "ITERATION:  63\n",
            "ITERATION:  64\n",
            "ITERATION:  65\n",
            "ITERATION:  66\n",
            "ITERATION:  67\n",
            "ITERATION:  68\n",
            "ITERATION:  69\n",
            "ITERATION:  70\n",
            "ITERATION:  71\n",
            "ITERATION:  72\n",
            "ITERATION:  73\n",
            "ITERATION:  74\n",
            "ITERATION:  75\n",
            "ITERATION:  76\n",
            "ITERATION:  77\n",
            "ITERATION:  78\n",
            "ITERATION:  79\n",
            "ITERATION:  80\n",
            "ITERATION:  81\n",
            "ITERATION:  82\n",
            "ITERATION:  83\n",
            "ITERATION:  84\n",
            "ITERATION:  85\n",
            "ITERATION:  86\n",
            "ITERATION:  87\n",
            "ITERATION:  88\n",
            "ITERATION:  89\n",
            "ITERATION:  90\n",
            "ITERATION:  91\n",
            "ITERATION:  92\n",
            "ITERATION:  93\n",
            "ITERATION:  94\n",
            "ITERATION:  95\n",
            "ITERATION:  96\n",
            "ITERATION:  97\n",
            "ITERATION:  98\n",
            "ITERATION:  99\n",
            "ITERATION:  100\n",
            "ITERATION:  101\n",
            "ITERATION:  102\n",
            "ITERATION:  103\n",
            "ITERATION:  104\n",
            "ITERATION:  105\n",
            "ITERATION:  106\n",
            "ITERATION:  107\n",
            "ITERATION:  108\n",
            "ITERATION:  109\n",
            "ITERATION:  110\n",
            "ITERATION:  111\n",
            "ITERATION:  112\n",
            "ITERATION:  113\n",
            "ITERATION:  114\n",
            "ITERATION:  115\n",
            "ITERATION:  116\n",
            "ITERATION:  117\n",
            "ITERATION:  118\n",
            "ITERATION:  119\n",
            "ITERATION:  120\n",
            "ITERATION:  121\n",
            "ITERATION:  122\n",
            "ITERATION:  123\n",
            "ITERATION:  124\n",
            "ITERATION:  125\n",
            "ITERATION:  126\n",
            "ITERATION:  127\n",
            "ITERATION:  128\n",
            "ITERATION:  129\n",
            "ITERATION:  130\n",
            "ITERATION:  131\n",
            "ITERATION:  132\n",
            "ITERATION:  133\n",
            "ITERATION:  134\n",
            "ITERATION:  135\n",
            "ITERATION:  136\n",
            "ITERATION:  137\n",
            "ITERATION:  138\n",
            "ITERATION:  139\n",
            "ITERATION:  140\n",
            "ITERATION:  141\n",
            "ITERATION:  142\n",
            "ITERATION:  143\n",
            "ITERATION:  144\n",
            "ITERATION:  145\n",
            "ITERATION:  146\n",
            "ITERATION:  147\n",
            "ITERATION:  148\n",
            "ITERATION:  149\n",
            "ITERATION:  150\n",
            "ITERATION:  151\n",
            "ITERATION:  152\n",
            "ITERATION:  153\n",
            "ITERATION:  154\n",
            "ITERATION:  155\n",
            "ITERATION:  156\n",
            "ITERATION:  157\n",
            "ITERATION:  158\n",
            "ITERATION:  159\n",
            "ITERATION:  160\n",
            "ITERATION:  161\n",
            "ITERATION:  162\n",
            "ITERATION:  163\n",
            "ITERATION:  164\n",
            "ITERATION:  165\n",
            "ITERATION:  166\n",
            "ITERATION:  167\n",
            "ITERATION:  168\n",
            "ITERATION:  169\n",
            "ITERATION:  170\n",
            "ITERATION:  171\n",
            "ITERATION:  172\n",
            "ITERATION:  173\n",
            "ITERATION:  174\n",
            "ITERATION:  175\n",
            "ITERATION:  176\n",
            "ITERATION:  177\n",
            "ITERATION:  178\n",
            "ITERATION:  179\n",
            "ITERATION:  180\n",
            "ITERATION:  181\n",
            "ITERATION:  182\n",
            "ITERATION:  183\n",
            "ITERATION:  184\n",
            "ITERATION:  185\n",
            "ITERATION:  186\n",
            "ITERATION:  187\n",
            "ITERATION:  188\n",
            "ITERATION:  189\n",
            "ITERATION:  190\n",
            "ITERATION:  191\n",
            "ITERATION:  192\n",
            "ITERATION:  193\n",
            "ITERATION:  194\n",
            "ITERATION:  195\n",
            "ITERATION:  196\n",
            "ITERATION:  197\n",
            "ITERATION:  198\n",
            "ITERATION:  199\n",
            "ITERATION:  200\n",
            "ITERATION:  201\n",
            "ITERATION:  202\n",
            "ITERATION:  203\n",
            "ITERATION:  204\n",
            "ITERATION:  205\n",
            "ITERATION:  206\n",
            "ITERATION:  207\n",
            "ITERATION:  208\n",
            "ITERATION:  209\n",
            "ITERATION:  210\n",
            "ITERATION:  211\n",
            "ITERATION:  212\n",
            "ITERATION:  213\n",
            "ITERATION:  214\n",
            "ITERATION:  215\n",
            "ITERATION:  216\n",
            "ITERATION:  217\n",
            "ITERATION:  218\n",
            "ITERATION:  219\n",
            "ITERATION:  220\n",
            "ITERATION:  221\n",
            "ITERATION:  222\n",
            "ITERATION:  223\n",
            "ITERATION:  224\n",
            "ITERATION:  225\n",
            "ITERATION:  226\n",
            "ITERATION:  227\n",
            "ITERATION:  228\n",
            "ITERATION:  229\n",
            "ITERATION:  230\n",
            "ITERATION:  231\n",
            "ITERATION:  232\n",
            "ITERATION:  233\n",
            "ITERATION:  234\n",
            "ITERATION:  235\n",
            "ITERATION:  236\n",
            "ITERATION:  237\n",
            "ITERATION:  238\n",
            "ITERATION:  239\n",
            "ITERATION:  240\n",
            "ITERATION:  241\n",
            "ITERATION:  242\n",
            "ITERATION:  243\n",
            "ITERATION:  244\n",
            "ITERATION:  245\n",
            "ITERATION:  246\n",
            "ITERATION:  247\n",
            "ITERATION:  248\n",
            "ITERATION:  249\n",
            "ITERATION:  250\n",
            "ITERATION:  251\n",
            "ITERATION:  252\n",
            "ITERATION:  253\n",
            "ITERATION:  254\n",
            "ITERATION:  255\n",
            "ITERATION:  256\n",
            "ITERATION:  257\n",
            "ITERATION:  258\n",
            "ITERATION:  259\n",
            "ITERATION:  260\n",
            "ITERATION:  261\n",
            "ITERATION:  262\n",
            "ITERATION:  263\n",
            "ITERATION:  264\n",
            "ITERATION:  265\n",
            "ITERATION:  266\n",
            "ITERATION:  267\n",
            "ITERATION:  268\n",
            "ITERATION:  269\n",
            "ITERATION:  270\n",
            "ITERATION:  271\n",
            "Accuracy@10: 0.5846\n",
            "Precision@10: 0.2343\n",
            "MRR@10: 0.3001\n",
            "MAP@10: 0.2706\n",
            "nDCG@10: 0.2244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "\n",
        "print(f'Accuracy@{k}: {calculate_accuracy(data, k):.4f}')\n",
        "print(f'Precision@{k}: {calculate_precision(data, k):.4f}')\n",
        "#print(f'Precision2@{k}: {calculate_precision2(data, k):.4f}')\n",
        "print(f'MRR@{k}: {calculate_mrr(data, k):.4f}')\n",
        "print(f'MAP@{k}: {calculate_map(data, k):.4f}')\n",
        "print(f'nDCG@{k}: {calculate_ndcg(data, k):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO9NKJLOfuEI",
        "outputId": "4247d335-e13e-4eec-c881-ad16c2cb361d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy@5: 0.4301\n",
            "Precision@5: 0.1417\n",
            "MRR@5: 0.2795\n",
            "MAP@5: 0.2698\n",
            "nDCG@5: 0.1880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 1\n",
        "\n",
        "print(f'Accuracy@{k}: {calculate_accuracy(data, k):.4f}')\n",
        "print(f'Precision@{k}: {calculate_precision(data, k):.4f}')\n",
        "#print(f'Precision2@{k}: {calculate_precision2(data, k):.4f}')\n",
        "print(f'MRR@{k}: {calculate_mrr(data, k):.4f}')\n",
        "print(f'MAP@{k}: {calculate_map(data, k):.4f}')\n",
        "print(f'nDCG@{k}: {calculate_ndcg(data, k):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT5QVbcMfxTf",
        "outputId": "c2f5d1f1-0ad6-4a8c-d5b4-147f9f56c21b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy@1: 0.1985\n",
            "Precision@1: 0.0348\n",
            "MRR@1: 0.1985\n",
            "MAP@1: 0.1985\n",
            "nDCG@1: 0.0993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class BM25:\n",
        "    def __init__(self, documents, k1=1.2, b=0.75):\n",
        "        self.documents = documents\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.doc_count = len(documents) # 총 문서 갯수\n",
        "        self.avgdl = 0                  # 문서 평균 길이 \n",
        "        self.doc_freqs = []             # 각 문서 내 corpus 카운트 \n",
        "        self.idf = {}                   # corpus당 idf 값\n",
        "        self.doc_len = []               # 각 문서 길이\n",
        "        \n",
        "        self._calc_idf()\n",
        "\n",
        "        \n",
        "    def _calc_idf(self):\n",
        "        doc_tokenized_corpus = list(map(lambda s: ' '.join(mecab.morphs(s)), self.documents ))\n",
        "        doc_corpus = [token.split(\" \") for token in doc_tokenized_corpus]\n",
        "\n",
        "        # 문서별 IDF 계산\n",
        "        word_freq = defaultdict(int)\n",
        "        total_words = 0\n",
        "        for document in doc_corpus:\n",
        "            self.doc_len.append(len(document))\n",
        "            total_words += len(document)\n",
        "            \n",
        "            frequencies = defaultdict(int)\n",
        "            for word in document:\n",
        "                frequencies[word] += 1\n",
        "            \n",
        "            self.doc_freqs.append(frequencies)\n",
        "            \n",
        "            for word, freq in frequencies.items():\n",
        "                word_freq[word] += 1\n",
        "\n",
        "        self.avgdl = total_words / self.doc_count\n",
        "        \n",
        "        for word, freq in word_freq.items():\n",
        "            idf = math.log(1 + (self.doc_count - freq + 0.5) / (freq + 0.5))\n",
        "            self.idf[word] = idf\n",
        "\n",
        "            \n",
        "    def get_top_n(self, query, n=5):\n",
        "        query_morphs = mecab.morphs(query)\n",
        "        scores = self.get_scores(query_morphs)\n",
        "        top_n = np.argsort(scores)[::-1][:n]\n",
        "        return [self.documents[i] for i in top_n]\n",
        "\n",
        "\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        score = np.zeros(self.doc_count)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(corpus[q] or 0) for corpus in self.doc_freqs])\n",
        "            # TF 계산\n",
        "            try:\n",
        "                score += (self.idf[q] or 0) * (q_freq * (self.k1 + 1) /\n",
        "                                                   (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "            except Exception:\n",
        "                # 알수 없는 단어일 경우 패스 \n",
        "                pass\n",
        "        return score"
      ],
      "metadata": {
        "id": "g_w5XLQHK5hG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}