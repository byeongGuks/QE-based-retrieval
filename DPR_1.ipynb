{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igcGhiz08nuM",
        "outputId": "2c21631e-a29d-4da2-9725-9518e1ecd531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DPR'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 58 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (58/58), 89.89 KiB | 1.76 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/DPR.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rluEHSN9qHF",
        "outputId": "fea548ca-96ae-4cf7-b9f2-c50b5b4481db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DPR\n"
          ]
        }
      ],
      "source": [
        "%cd DPR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd DPR/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk3UkKGxsXCx",
        "outputId": "25257c28-b000-4e3b-dfe9-3e3e56c9d17c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mDPR\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKAd0PH--f8A",
        "outputId": "a21ec0d4-27d3-44bf-9996-c651dc80b66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools==58.2.0 in /usr/local/lib/python3.10/dist-packages (58.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install setuptools==58.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QqTVJxb84vF",
        "outputId": "f7d81d59-27cf-40a6-d533-15d865ff56df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating dpr.egg-info\n",
            "writing dpr.egg-info/PKG-INFO\n",
            "writing dependency_links to dpr.egg-info/dependency_links.txt\n",
            "writing requirements to dpr.egg-info/requires.txt\n",
            "writing top-level names to dpr.egg-info/top_level.txt\n",
            "writing manifest file 'dpr.egg-info/SOURCES.txt'\n",
            "reading manifest file 'dpr.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'dpr.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dpr.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dpr.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dpr.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dpr.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying dpr.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/dpr-1.0.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing dpr-1.0.0-py3.10.egg\n",
            "Copying dpr-1.0.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding dpr 1.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/dpr-1.0.0-py3.10.egg\n",
            "Processing dependencies for dpr==1.0.0\n",
            "Searching for jsonlines\n",
            "Reading https://pypi.org/simple/jsonlines/\n",
            "Downloading https://files.pythonhosted.org/packages/68/32/290ca20eb3a2b97ffa6ba1791fcafacb3cd2f41f539c96eb54cfc3cfcf47/jsonlines-3.1.0-py3-none-any.whl#sha256=632f5e38f93dfcb1ac8c4e09780b92af3a55f38f26e7c47ae85109d420b6ad39\n",
            "Best match: jsonlines 3.1.0\n",
            "Processing jsonlines-3.1.0-py3-none-any.whl\n",
            "Installing jsonlines-3.1.0-py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding jsonlines 3.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/jsonlines-3.1.0-py3.10.egg\n",
            "Searching for omegaconf>=2.0.1\n",
            "Reading https://pypi.org/simple/omegaconf/\n",
            "Downloading https://files.pythonhosted.org/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl#sha256=7b4df175cdb08ba400f45cae3bdcae7ba8365db4d165fc65fd04b050ab63b46b\n",
            "Best match: omegaconf 2.3.0\n",
            "Processing omegaconf-2.3.0-py3-none-any.whl\n",
            "Installing omegaconf-2.3.0-py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding omegaconf 2.3.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/omegaconf-2.3.0-py3.10.egg\n",
            "Searching for hydra-core>=1.0.0\n",
            "Reading https://pypi.org/simple/hydra-core/\n",
            "Downloading https://files.pythonhosted.org/packages/c6/50/e0edd38dcd63fb26a8547f13d28f7a008bc4a3fd4eb4ff030673f22ad41a/hydra_core-1.3.2-py3-none-any.whl#sha256=fa0238a9e31df3373b35b0bfb672c34cc92718d21f81311d8996a16de1141d8b\n",
            "Best match: hydra-core 1.3.2\n",
            "Processing hydra_core-1.3.2-py3-none-any.whl\n",
            "Installing hydra_core-1.3.2-py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding hydra-core 1.3.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/hydra_core-1.3.2-py3.10.egg\n",
            "Searching for wget\n",
            "Reading https://pypi.org/simple/wget/\n",
            "Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip#sha256=35e630eca2aa50ce998b9b1a127bb26b30dfee573702782aa982f875e3f16061\n",
            "Best match: wget 3.2\n",
            "Processing wget-3.2.zip\n",
            "Writing /tmp/easy_install-m8msxebq/wget-3.2/setup.cfg\n",
            "Running wget-3.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-m8msxebq/wget-3.2/egg-dist-tmp-iflbmpq_\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "Moving wget-3.2-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding wget 3.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/wget-3.2-py3.10.egg\n",
            "Searching for transformers>=4.3\n",
            "Reading https://pypi.org/simple/transformers/\n",
            "Downloading https://files.pythonhosted.org/packages/d8/a7/a6ff727fd5d96d6625f4658944a2ae230f0c75743a9a117fbda013b03d3d/transformers-4.28.1-py3-none-any.whl#sha256=f30a006220d0475789ac0e7c874f51bf5143956797616d89975b637883ce0be6\n",
            "Best match: transformers 4.28.1\n",
            "Processing transformers-4.28.1-py3-none-any.whl\n",
            "Installing transformers-4.28.1-py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding transformers 4.28.1 to easy-install.pth file\n",
            "Installing transformers-cli script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/transformers-4.28.1-py3.10.egg\n",
            "Searching for faiss-cpu>=1.6.1\n",
            "Reading https://pypi.org/simple/faiss-cpu/\n",
            "Downloading https://files.pythonhosted.org/packages/ac/a2/9f652d142cc3b70db10621d688dbd8ead162a74b7e0c8d7dda315c52ae4e/faiss-cpu-1.7.4.tar.gz#sha256=265dc31b0c079bf4433303bf6010f73922490adff9188b915e2d3f5e9c82dd0a\n",
            "Best match: faiss-cpu 1.7.4\n",
            "Processing faiss-cpu-1.7.4.tar.gz\n",
            "Writing /tmp/easy_install-hssimvu8/faiss-cpu-1.7.4/setup.cfg\n",
            "Running faiss-cpu-1.7.4/setup.py -q bdist_egg --dist-dir /tmp/easy_install-hssimvu8/faiss-cpu-1.7.4/egg-dist-tmp-1arsu3rv\n",
            "faiss/faiss/python/swigfaiss.i:268: Error: Unable to find 'faiss/impl/platform_macros.h'\n",
            "faiss/faiss/python/swigfaiss.i:272: Error: Unable to find 'faiss/utils/ordered_key_value.h'\n",
            "faiss/faiss/python/swigfaiss.i:273: Error: Unable to find 'faiss/utils/Heap.h'\n",
            "faiss/faiss/python/swigfaiss.i:278: Error: Unable to find 'faiss/utils/AlignedTable.h'\n",
            "faiss/faiss/python/swigfaiss.i:279: Error: Unable to find 'faiss/utils/partitioning.h'\n",
            "faiss/faiss/python/swigfaiss.i:280: Error: Unable to find 'faiss/utils/hamming.h'\n",
            "faiss/faiss/python/swigfaiss.i:281: Error: Unable to find 'faiss/utils/hamming_distance/common.h'\n",
            "faiss/faiss/python/swigfaiss.i:386: Error: Unable to find 'faiss/utils/utils.h'\n",
            "faiss/faiss/python/swigfaiss.i:387: Error: Unable to find 'faiss/utils/distances.h'\n",
            "faiss/faiss/python/swigfaiss.i:388: Error: Unable to find 'faiss/utils/random.h'\n",
            "faiss/faiss/python/swigfaiss.i:389: Error: Unable to find 'faiss/utils/sorting.h'\n",
            "faiss/faiss/python/swigfaiss.i:391: Error: Unable to find 'faiss/MetricType.h'\n",
            "faiss/faiss/python/swigfaiss.i:396: Error: Unable to find 'faiss/Index.h'\n",
            "faiss/faiss/python/swigfaiss.i:398: Error: Unable to find 'faiss/impl/DistanceComputer.h'\n",
            "faiss/faiss/python/swigfaiss.i:401: Error: Unable to find 'faiss/IndexFlatCodes.h'\n",
            "faiss/faiss/python/swigfaiss.i:402: Error: Unable to find 'faiss/IndexFlat.h'\n",
            "faiss/faiss/python/swigfaiss.i:403: Error: Unable to find 'faiss/Clustering.h'\n",
            "faiss/faiss/python/swigfaiss.i:405: Error: Unable to find 'faiss/utils/extra_distances.h'\n",
            "faiss/faiss/python/swigfaiss.i:409: Error: Unable to find 'faiss/impl/Quantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:410: Error: Unable to find 'faiss/impl/ProductQuantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:411: Error: Unable to find 'faiss/impl/AdditiveQuantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:412: Error: Unable to find 'faiss/impl/ResidualQuantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:413: Error: Unable to find 'faiss/impl/LocalSearchQuantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:414: Error: Unable to find 'faiss/impl/ProductAdditiveQuantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:415: Error: Unable to find 'faiss/impl/CodePacker.h'\n",
            "faiss/faiss/python/swigfaiss.i:417: Error: Unable to find 'faiss/VectorTransform.h'\n",
            "faiss/faiss/python/swigfaiss.i:418: Error: Unable to find 'faiss/IndexPreTransform.h'\n",
            "faiss/faiss/python/swigfaiss.i:419: Error: Unable to find 'faiss/IndexRefine.h'\n",
            "faiss/faiss/python/swigfaiss.i:420: Error: Unable to find 'faiss/IndexLSH.h'\n",
            "faiss/faiss/python/swigfaiss.i:421: Error: Unable to find 'faiss/impl/PolysemousTraining.h'\n",
            "faiss/faiss/python/swigfaiss.i:422: Error: Unable to find 'faiss/IndexPQ.h'\n",
            "faiss/faiss/python/swigfaiss.i:423: Error: Unable to find 'faiss/IndexAdditiveQuantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:424: Error: Unable to find 'faiss/impl/io.h'\n",
            "faiss/faiss/python/swigfaiss.i:426: Error: Unable to find 'faiss/invlists/InvertedLists.h'\n",
            "faiss/faiss/python/swigfaiss.i:427: Error: Unable to find 'faiss/invlists/InvertedListsIOHook.h'\n",
            "faiss/faiss/python/swigfaiss.i:429: Error: Unable to find 'faiss/invlists/BlockInvertedLists.h'\n",
            "faiss/faiss/python/swigfaiss.i:430: Error: Unable to find 'faiss/invlists/DirectMap.h'\n",
            "faiss/faiss/python/swigfaiss.i:431: Error: Unable to find 'faiss/IndexIVF.h'\n",
            "faiss/faiss/python/swigfaiss.i:436: Error: Unable to find 'faiss/IVFlib.h'\n",
            "faiss/faiss/python/swigfaiss.i:437: Error: Unable to find 'faiss/impl/ScalarQuantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:438: Error: Unable to find 'faiss/IndexScalarQuantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:439: Error: Unable to find 'faiss/IndexIVFSpectralHash.h'\n",
            "faiss/faiss/python/swigfaiss.i:440: Error: Unable to find 'faiss/IndexIVFAdditiveQuantizer.h'\n",
            "faiss/faiss/python/swigfaiss.i:441: Error: Unable to find 'faiss/impl/HNSW.h'\n",
            "faiss/faiss/python/swigfaiss.i:442: Error: Unable to find 'faiss/IndexHNSW.h'\n",
            "faiss/faiss/python/swigfaiss.i:444: Error: Unable to find 'faiss/impl/kmeans1d.h'\n",
            "faiss/faiss/python/swigfaiss.i:447: Error: Unable to find 'faiss/impl/NNDescent.h'\n",
            "faiss/faiss/python/swigfaiss.i:448: Error: Unable to find 'faiss/IndexNNDescent.h'\n",
            "faiss/faiss/python/swigfaiss.i:450: Error: Unable to find 'faiss/IndexIVFFlat.h'\n",
            "faiss/faiss/python/swigfaiss.i:451: Error: Unable to find 'faiss/impl/NSG.h'\n",
            "faiss/faiss/python/swigfaiss.i:452: Error: Unable to find 'faiss/IndexNSG.h'\n",
            "faiss/faiss/python/swigfaiss.i:457: Error: Unable to find 'faiss/invlists/OnDiskInvertedLists.h'\n",
            "faiss/faiss/python/swigfaiss.i:460: Error: Unable to find 'faiss/impl/lattice_Zn.h'\n",
            "faiss/faiss/python/swigfaiss.i:461: Error: Unable to find 'faiss/IndexLattice.h'\n",
            "faiss/faiss/python/swigfaiss.i:464: Error: Unable to find 'faiss/IndexIVFPQ.h'\n",
            "faiss/faiss/python/swigfaiss.i:465: Error: Unable to find 'faiss/IndexIVFPQR.h'\n",
            "faiss/faiss/python/swigfaiss.i:466: Error: Unable to find 'faiss/Index2Layer.h'\n",
            "faiss/faiss/python/swigfaiss.i:468: Error: Unable to find 'faiss/IndexFastScan.h'\n",
            "faiss/faiss/python/swigfaiss.i:469: Error: Unable to find 'faiss/IndexAdditiveQuantizerFastScan.h'\n",
            "faiss/faiss/python/swigfaiss.i:470: Error: Unable to find 'faiss/IndexPQFastScan.h'\n",
            "faiss/faiss/python/swigfaiss.i:471: Error: Unable to find 'faiss/IndexIVFFastScan.h'\n",
            "faiss/faiss/python/swigfaiss.i:472: Error: Unable to find 'faiss/IndexIVFAdditiveQuantizerFastScan.h'\n",
            "faiss/faiss/python/swigfaiss.i:473: Error: Unable to find 'faiss/IndexIVFPQFastScan.h'\n",
            "faiss/faiss/python/swigfaiss.i:474: Error: Unable to find 'faiss/utils/quantize_lut.h'\n",
            "faiss/faiss/python/swigfaiss.i:476: Error: Unable to find 'faiss/IndexBinary.h'\n",
            "faiss/faiss/python/swigfaiss.i:477: Error: Unable to find 'faiss/IndexBinaryFlat.h'\n",
            "faiss/faiss/python/swigfaiss.i:478: Error: Unable to find 'faiss/IndexBinaryIVF.h'\n",
            "faiss/faiss/python/swigfaiss.i:479: Error: Unable to find 'faiss/IndexBinaryFromFloat.h'\n",
            "faiss/faiss/python/swigfaiss.i:480: Error: Unable to find 'faiss/IndexBinaryHNSW.h'\n",
            "faiss/faiss/python/swigfaiss.i:481: Error: Unable to find 'faiss/IndexBinaryHash.h'\n",
            "faiss/faiss/python/swigfaiss.i:483: Error: Unable to find 'faiss/impl/ThreadedIndex.h'\n",
            "faiss/faiss/python/swigfaiss.i:487: Error: Unable to find 'faiss/IndexShards.h'\n",
            "faiss/faiss/python/swigfaiss.i:490: Error: Unable to find 'faiss/IndexShardsIVF.h'\n",
            "faiss/faiss/python/swigfaiss.i:492: Error: Unable to find 'faiss/IndexReplicas.h'\n",
            "faiss/faiss/python/swigfaiss.i:496: Error: Unable to find 'faiss/MetaIndexes.h'\n",
            "faiss/faiss/python/swigfaiss.i:497: Error: Unable to find 'faiss/IndexIDMap.h'\n",
            "faiss/faiss/python/swigfaiss.i:503: Error: Unable to find 'faiss/IndexRowwiseMinMax.h'\n",
            "faiss/faiss/python/swigfaiss.i:513: Error: Unable to find 'faiss/impl/AuxIndexStructures.h'\n",
            "faiss/faiss/python/swigfaiss.i:514: Error: Unable to find 'faiss/impl/IDSelector.h'\n",
            "faiss/faiss/python/swigfaiss.i:516: Error: Unable to find 'faiss/utils/approx_topk/mode.h'\n",
            "faiss/faiss/python/swigfaiss.i:753: Error: Unable to find 'faiss/index_io.h'\n",
            "faiss/faiss/python/swigfaiss.i:754: Error: Unable to find 'faiss/clone_index.h'\n",
            "faiss/faiss/python/swigfaiss.i:758: Error: Unable to find 'faiss/AutoTune.h'\n",
            "faiss/faiss/python/swigfaiss.i:759: Error: Unable to find 'faiss/index_factory.h'\n",
            "faiss/faiss/python/swigfaiss.i:760: Error: Unable to find 'faiss/MatrixStats.h'\n",
            "error: Setup script exited with error: command '/usr/local/bin/swig' failed with exit code 1\n"
          ]
        }
      ],
      "source": [
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju5cMdvKOfKA",
        "outputId": "6c0c44f8-d199-4e1c-8162-c72cb22d3ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CJYT9m-Oul3",
        "outputId": "daef6ddb-2c74-4c7a-ce31-7a9318eb12b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.3 in /usr/local/lib/python3.10/dist-packages (4.9.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install antlr4-python3-runtime==4.9.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zr3WkTVPwdu",
        "outputId": "4509f02f-5432-4510-f85c-0814ccc410cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpF-CpIl9opM"
      },
      "outputs": [],
      "source": [
        "# !python dpr/data/download_data.py\n",
        "\t# --resource {key from download_data.py's RESOURCES_MAP}  \\\n",
        "\t# [optional --output_dir {your location}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXE92KbiNXOG"
      },
      "outputs": [],
      "source": [
        "!python dpr/data/download_data.py --resource data.retriever.nq-train\n",
        "# data.retriever.nq-train\n",
        "# data.retriever.nq-dev\n",
        "# data.retriever.qas.nq-train\n",
        "# data.retriever.qas.nq-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt-IXGpTNuNK",
        "outputId": "73fa4b1c-041a-41da-c3e6-4e470071c4cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-09 08:49:31.354910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-09 08:49:32.282105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-09 08:49:33.581199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-09 08:49:33.581738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-09 08:49:33.581987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "/content/DPR/DPR-main/train_dense_encoder.py:747: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @hydra.main(config_path=\"conf\", config_name=\"biencoder_train_cfg\")\n",
            "[139907146504000] 2023-05-09 08:49:39,457 [INFO] root: Sys.argv: ['train_dense_encoder.py', 'train_datasets=[nq_train_hn1]', 'dev_datasets=[nq_dev]', 'train=biencoder_local', 'output_dir=./', '+batch_size=256']\n",
            "[139907146504000] 2023-05-09 08:49:39,457 [INFO] root: Hydra formatted Sys.argv: ['train_dense_encoder.py', 'train_datasets=[nq_train_hn1]', 'dev_datasets=[nq_dev]', 'train=biencoder_local', 'output_dir=./', '+batch_size=256']\n",
            "/usr/local/lib/python3.10/dist-packages/hydra_core-1.3.2-py3.10.egg/hydra/_internal/defaults_list.py:251: UserWarning: In 'biencoder_train_cfg': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/hydra_core-1.3.2-py3.10.egg/hydra/core/default_element.py:124: UserWarning: In 'datasets/encoder_train_default': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  deprecation_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/hydra_core-1.3.2-py3.10.egg/hydra/core/default_element.py:124: UserWarning: In 'train/biencoder_local': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  deprecation_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/hydra_core-1.3.2-py3.10.egg/hydra/core/default_element.py:124: UserWarning: In 'encoder/hf_bert': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  deprecation_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/hydra_core-1.3.2-py3.10.egg/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2023-05-09 08:49:39,898][root][INFO] - CFG's local_rank=-1\n",
            "[2023-05-09 08:49:39,899][root][INFO] - Env WORLD_SIZE=None\n",
            "[2023-05-09 08:49:39,900][root][INFO] - Initialized host 3f168a655278 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
            "[2023-05-09 08:49:39,900][root][INFO] - 16-bits training: False \n",
            "[2023-05-09 08:49:39,901][root][INFO] - CFG (after gpu  configuration):\n",
            "[2023-05-09 08:49:39,911][root][INFO] - encoder:\n",
            "  encoder_model_type: hf_bert\n",
            "  pretrained_model_cfg: bert-base-uncased\n",
            "  pretrained_file: null\n",
            "  projection_dim: 0\n",
            "  sequence_length: 256\n",
            "  dropout: 0.1\n",
            "  fix_ctx_encoder: false\n",
            "  pretrained: true\n",
            "train:\n",
            "  batch_size: 1\n",
            "  dev_batch_size: 16\n",
            "  adam_eps: 1.0e-08\n",
            "  adam_betas: (0.9, 0.999)\n",
            "  max_grad_norm: 2.0\n",
            "  log_batch_step: 1\n",
            "  train_rolling_loss_step: 100\n",
            "  weight_decay: 0.0\n",
            "  learning_rate: 2.0e-05\n",
            "  warmup_steps: 1237\n",
            "  gradient_accumulation_steps: 1\n",
            "  num_train_epochs: 40\n",
            "  eval_per_epoch: 1\n",
            "  hard_negatives: 1\n",
            "  other_negatives: 0\n",
            "  val_av_rank_hard_neg: 30\n",
            "  val_av_rank_other_neg: 30\n",
            "  val_av_rank_bsz: 128\n",
            "  val_av_rank_max_qs: 10000\n",
            "datasets:\n",
            "  nq_train:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.nq-train\n",
            "  nq_train_hn1:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.nq-adv-hn-train\n",
            "  nq_dev:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.nq-dev\n",
            "  trivia_train:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.trivia-train\n",
            "  trivia_dev:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.trivia-dev\n",
            "  squad1_train:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.squad1-train\n",
            "  squad1_dev:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.squad1-dev\n",
            "  webq_train:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.webq-train\n",
            "  webq_dev:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.webq-dev\n",
            "  curatedtrec_train:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.curatedtrec-train\n",
            "  curatedtrec_dev:\n",
            "    _target_: dpr.data.biencoder_data.JsonQADataset\n",
            "    file: data.retriever.curatedtrec-dev\n",
            "train_datasets:\n",
            "- nq_train_hn1\n",
            "dev_datasets:\n",
            "- nq_dev\n",
            "output_dir: ./\n",
            "train_sampling_rates: null\n",
            "loss_scale_factors: null\n",
            "do_lower_case: true\n",
            "val_av_rank_start_epoch: 30\n",
            "seed: 12345\n",
            "checkpoint_file_name: dpr_biencoder\n",
            "model_file: null\n",
            "local_rank: -1\n",
            "global_loss_buf_sz: 592000\n",
            "device: cuda\n",
            "distributed_world_size: 1\n",
            "distributed_port: null\n",
            "distributed_init_method: null\n",
            "no_cuda: false\n",
            "n_gpu: 1\n",
            "fp16: false\n",
            "fp16_opt_level: O1\n",
            "special_tokens: null\n",
            "ignore_checkpoint_offset: false\n",
            "ignore_checkpoint_optimizer: false\n",
            "ignore_checkpoint_lr: false\n",
            "multi_q_encoder: false\n",
            "local_shards_dataloader: false\n",
            "batch_size: 256\n",
            "\n",
            "[2023-05-09 08:49:39,911][root][INFO] - ***** Initializing components for training *****\n",
            "[2023-05-09 08:49:39,912][root][INFO] - Checkpoint files []\n",
            "[2023-05-09 08:49:40,178][dpr.models.hf_models][INFO] - Initializing HF BERT Encoder. cfg_name=bert-base-uncased\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing HFBertEncoder: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing HFBertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HFBertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-05-09 08:49:41,816][dpr.models.hf_models][INFO] - Initializing HF BERT Encoder. cfg_name=bert-base-uncased\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing HFBertEncoder: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing HFBertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HFBertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers-4.28.1-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[2023-05-09 08:49:43,399][dpr.utils.conf_utils][INFO] - train_datasets: ['nq_train_hn1']\n",
            "[2023-05-09 08:49:43,401][dpr.utils.conf_utils][INFO] - dev_datasets: ['nq_dev']\n",
            "[2023-05-09 08:49:43,402][root][INFO] - Initializing task/set data ['nq_train_hn1']\n",
            "[2023-05-09 08:49:43,402][root][INFO] - Calculating shard positions\n",
            "[2023-05-09 08:49:43,402][dpr.data.biencoder_data][INFO] - Loading all data\n",
            "[2023-05-09 08:49:43,409][dpr.data.download_data][INFO] - matched by prefix resources: []\n",
            "[2023-05-09 08:49:43,409][dpr.data.download_data][INFO] - no resources found for specified key\n",
            "[2023-05-09 08:49:43,409][dpr.data.biencoder_data][INFO] - Data files: []\n",
            "[2023-05-09 08:49:43,409][dpr.data.biencoder_data][INFO] - Total cleaned data size: 0\n",
            "[2023-05-09 08:49:43,409][root][INFO] - samples_per_shard=0, shard_start_idx=0, shard_end_idx=0, max_iterations=0\n",
            "[2023-05-09 08:49:43,410][dpr.data.download_data][INFO] - matched by prefix resources: []\n",
            "[2023-05-09 08:49:43,410][dpr.data.download_data][INFO] - no resources found for specified key\n",
            "[2023-05-09 08:49:43,410][dpr.data.biencoder_data][INFO] - Data files: []\n",
            "[2023-05-09 08:49:43,410][dpr.data.biencoder_data][INFO] - Total cleaned data size: 0\n",
            "[2023-05-09 08:49:43,410][root][INFO] - Sharded dataset data 0\n",
            "[2023-05-09 08:49:43,410][root][INFO] - rank=-1; Multi set data sizes [0]\n",
            "[2023-05-09 08:49:43,410][root][INFO] - rank=-1; Multi set total data 0\n",
            "[2023-05-09 08:49:43,410][root][INFO] - rank=-1; Multi set sampling_rates None\n",
            "[2023-05-09 08:49:43,410][root][INFO] - rank=-1; Multi set max_iterations per dataset [0]\n",
            "[2023-05-09 08:49:43,410][root][INFO] - rank=-1; Multi set max_iterations 0\n",
            "[2023-05-09 08:49:43,410][root][INFO] -   Total iterations per epoch=0\n",
            "[2023-05-09 08:49:43,410][root][WARNING] - No data found for training.\n"
          ]
        }
      ],
      "source": [
        "!python train_dense_encoder.py \\\n",
        "train_datasets=[nq_train_hn1] \\\n",
        "dev_datasets=[nq_dev] \\\n",
        "train=biencoder_local \\\n",
        "output_dir='./' \\\n",
        "+batch_size=256\n",
        "# too many time "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Drive/MyDrive/triviaqa-unfiltered"
      ],
      "metadata": {
        "id": "lHPKruIcNVBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49dd0ae2-5a20-4d3c-c7fd-330ca388bda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/Drive/MyDrive/triviaqa-unfiltered'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eV-A_IWOXBS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# JSON 파일 경로\n",
        "file_path = \"\"\n",
        "\n",
        "# JSON 파일 열기\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
        "    # JSON 파일 읽어오기\n",
        "    json_data = json.load(json_file)\n",
        "    \n",
        "    # JSON 데이터에서 \"id\" 값 추출\n",
        "    for item in json_data:\n",
        "        id_value = item[\"id\"]\n",
        "        print(id_value)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}